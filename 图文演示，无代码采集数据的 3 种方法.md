---
title: "图文演示，无代码采集数据的 3 种方法"
seoDescription: "用八爪鱼之类的工具采集数据是我学会的最实用的技能，花几个小时捋一捋流程，不用写代码，终生受益。"
datePublished: Sat Oct 19 2024 10:00:14 GMT+0000 (Coordinated Universal Time)
cuid: cm2fzmpxh000209l7gpxs4ihx
slug: 20241019175947
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1729331650129/d10a1238-5525-4f19-bb89-1250250798b9.jpeg
tags: 5bel5yw3566x

---

来交技能篇[读者翻的牌子](https://mp.weixin.qq.com/s?__biz=MzI3MzU5MDA1OQ==&mid=2247488539&idx=1&sn=a65ae13d70f465a695bff61ab1fd054f&scene=21#wechat_redirect)了。先写评论点名最多的三篇：《怎么用八爪鱼爬数据》《怎么听懂英文播客》《怎么做无实体副业》。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331680325/97242ac0-3cb6-4e50-b14c-7cb1f4d4e036.png)

生活中总会想「我要是有这些数据就好了」的时刻。不管是剧荒时参考豆瓣电影 Top 250 榜单，还是批量拉取某个公众号的文章标题，又或者分析商家爆品销量和评论。

不管是手动复制粘贴，还是用八爪鱼、后裔采集器、影刀 RPA，甚至自己写爬虫，都只是获取数据的方法之一。有时候手动快，有时候非工具不可。分享我应对不同场景获取数据的方法和思路。

### Chrome 插件

每个月我会手动把最新博文更新到 [GitHub](https://github.com/CaiGeen/Hashnode-blog)，一个个复制粘贴链接太笨了，设置 Actions 自动同步当然酷，但我不会。所以有没有更合适我的方法？用八爪鱼？可以，但太重，就为了不到 10 条数据还得开软件。有没有更轻便的？有，Chrome 浏览器插件。

打开 [Chrome 应用商店](https://chromewebstore.google.com/?hl=zh-CN)，搜索「爬虫」或「spider」，找好评多的挨个试试，看能不能满足你的需求。别怕难，浏览器插件一般都是傻瓜式操作。比如我正在用「[Instant Data Scraper](https://chromewebstore.google.com/detail/ofaokhiedipichpaobibbnahnkdoiiah)」，截图演示一下。

1. 打开网站，点击 Instant Data Scraper
    
2. 如果数据不是自己想要的，点击“Try another table”
    
3. 复制或者下载 .xlsx 文件到本地处理数据
    

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331733938/40da21a3-3253-464e-8f20-8e937088021b.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331798238/7217d14b-f9a4-4309-9427-891b881818ef.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331758332/03bc7c46-e042-4523-a2e5-13a7e51c9ed0.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331780447/e99ebfc8-c612-472c-8c6e-25d74c7b20a8.png)

再用豆瓣电影 Top 250 试试。

这次运气不错，打开软件数据就是标准的。但我只想要电影链接、标题、评分和评分人数，其他多余的数据直接叉掉。叉错了，可以点击“Reset columns”按钮恢复数据列。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331835213/1e8d030f-4e0f-46cb-9943-71c119000a4a.png)

稍微进阶一点儿，比如你想抓更多数据，需要翻页或者无限滚动，可以选中「定位下一页」按钮，或者勾选「无限滚动」框。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331840445/2d4281e2-3fa8-4a2e-b546-d883c41dfa2d.png)

### 八爪鱼采集器

总有用「Instant Data Scraper」无效，或者浏览器没有插件功能装不了这些扩展的时候。那就稍微麻烦点儿，装个八爪鱼采集器吧。

下载八爪鱼采集器，安装[注册](https://affiliate.bazhuayu.com/f3dzG6)（注册时填邀请码：f3dzG6，我会得到 1 元奖励谢谢老板）登录后，输入你想抓数据的网站链接，开始采集。

换张潇雨的微博举例。

1. 在八爪鱼中输入张潇雨的微博链接
    
2. 点击「自动识别网页数据」
    
3. 删减你不需要的数据字段
    
4. 点击右上角「采集」，选择本地采集
    

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331852937/3f4ac162-04c5-49a9-b85c-b2de29746c1c.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331858617/29418dd4-e53c-4302-aa00-b745d54a15dc.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331861944/08810605-070b-4b41-a668-c94322823664.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331866149/2b2e0998-415a-488b-8da1-102de3489ea0.png)

到这里看起来都很像「Instant Data Scraper」，但如果你想自由组合数据字段，可以不用自动识别网页，手动点击你要的数据，然后选择「选中全部相似元素」，这个字段就被添加到采集需求框了。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331889945/419ac879-3f17-455d-abd7-84c77a474fa8.png)

再进一步。假如你想重复搜索一批数据的结果，比如你问朋友有什么小说好看，他丢给你了一堆书名。你想知道哪本好评更多，手动的话，就要一本本去起点或者豆瓣搜评分。但如果用八爪鱼就方便多了。

1. 准备好批量搜索的文本
    
2. 输入起点中文网搜索链接
    
3. 点击搜索框，选择「输入文本」
    
4. 选择「批量输入文本」，粘贴搜索词
    
5. 网页点击搜索按钮，选择「点击该链接」
    
6. 逐个点选我要的数据字段
    
7. 点击右上角「采集」，选择本地采集
    

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331912170/a25366c8-27f1-4286-97b0-92ee8e54d338.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331915565/d17d0240-00af-4f80-8978-ca91f813386f.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331918947/94a15b70-9406-4d52-bd47-2293e1479487.png)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729331926055/7e3897c9-f14b-425e-be80-35665bc9b20d.png)

如果我没讲清楚，这有官方的[图文教程](https://www.bazhuayu.com/helpcenter/docs/2DUt4F)。

到这一步，采集大多网页数据已不在话下。还有更高的要求，比如采集每条微博的标准格式时间，或者采集小宇宙 APP 的播客评论，就需要会点儿代码，或者找帮手了。

### 万能淘宝

向熟人求助总难以启齿，怕自己需求小担心劳对方大驾。就算你说给钱，也不知道给多少，多了少了都不合适。不如直接淘宝。

上淘宝搜索你要搜索的平台+数据/爬虫之类的关键词，比如「公众号数据」，或者直接搜「数据服务、数据爬虫、Python 爬虫」。唤起客服，和他聊聊你的需求，等他报价，如果合适，就下单坐等，啥也不操心。

用八爪鱼之类的工具采集数据是我学会的最实用的技能，花几个小时捋一捋流程，不用写代码，终生受益。如果你也学会了，欢迎回来报喜，顺便说说你都采集了什么有意思的数据（但不要随便分享可能侵权 😎）

🔗

《[AI 来了，什么技能最值得我们学](https://mp.weixin.qq.com/s?__biz=MzI3MzU5MDA1OQ==&mid=2247487648&idx=1&sn=d86ff126d81bdd53d299a6c7eeb92ec1&chksm=eb21a2e4dc562bf2c71eb5e8f7eb0fa3ef77186a5b4e182d6b51554cd4e41aa5f226400776dc#rd)》

《[工作近 10 年，我靠这 9 个技能包](https://mp.weixin.qq.com/s?__biz=MzI3MzU5MDA1OQ==&mid=2247488539&idx=1&sn=a65ae13d70f465a695bff61ab1fd054f&chksm=eb21a65fdc562f49513235d8a9e3285120a60e0e896902a087bd62d3b74dce1e85fcf311c4fd#rd)》

💳 支持我并订阅我的[动态和读书笔记](https://mp.weixin.qq.com/s/A_yK10ktL8Nl7RzsnGwzEg)，请购买我的 Telegram 频道，现价 499 元，我会用这份收入采集更多内容，用创作反哺。